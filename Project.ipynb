{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer #import the French stemming library\n",
    "from nltk.corpus import stopwords #import stopwords from nltk corpus\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# create a frequency table for all words\n",
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# Remove stop words from English\n",
    "def removeStopWordsEnglish(lines):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab\n",
    "\n",
    "def removeStopWordsFrench(lines):\n",
    "    stopWords = set(stopwords.words('french'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab\n",
    "\n",
    "# Remove stop words from Romanian\n",
    "def removeStopWordsRomanian(lines):\n",
    "    wordsFiltered = []\n",
    "    stopWords = set(stopwords.words('romanian'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab\n",
    "\n",
    "# remove all words with a frequency below a threshold\n",
    "def trim_vocab(vocab, min_occurance):\n",
    "    tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "    return set(tokens)\n",
    "\n",
    "# mark all OOV with \"unk\" for all lines\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append('unk')\n",
    "        new_line = ' '.join(new_tokens)\n",
    "        new_lines.append(new_line)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english.pkl\n",
      "English Vocabulary: 46454\n",
      "English Sentences: 399042\n",
      "### After removing the stop words ###\n",
      "English Vocabulary: 46318\n",
      "English Sentences: 399042\n",
      "### After Pruning the data ###\n",
      "New English Vocabulary: 19981\n",
      "New English Sentences: 399042\n",
      "Saved: english_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = 'K:\\IIT Life\\Lecture Notes\\SEM 2 - OSNA DAA ADM\\ADM\\Project Reports\\Europal DS\\en-ro.txt\\Europarl.en-ro.en'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "sentences = clean_lines(sentences)\n",
    "save_clean_sentences(sentences, 'english.pkl')\n",
    "\n",
    "# load English dataset\n",
    "filename = 'english.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "print('English Sentences: %d' % len(lines))\n",
    "\n",
    "# removing stop words from vocabulary\n",
    "vocab = removeStopWordsEnglish(lines)\n",
    "print ('### After removing the stop words ###')\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "print('English Sentences: %d' % len(lines))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print ('### After Pruning the data ###')\n",
    "print('New English Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "print('New English Sentences: %d' % len(lines))\n",
    "# save updated dataset\n",
    "filename = 'english_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: roman.pkl\n",
      "Roman Vocabulary: 71020\n",
      "Roman Sentences: 399042\n",
      "### After removing the stop words ###\n",
      "Roman Vocabulary: 70759\n",
      "Roman Sentences: 399042\n",
      "### After Pruning the data ###\n",
      "New Roman Vocabulary: 30865\n",
      "New Roman Sentences: 399042\n",
      "Saved: roman_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# load Roman data\n",
    "filename = 'K:\\IIT Life\\Lecture Notes\\SEM 2 - OSNA DAA ADM\\ADM\\Project Reports\\Europal DS\\en-ro.txt\\Europarl.en-ro.ro'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "sentences = clean_lines(sentences)\n",
    "save_clean_sentences(sentences, 'roman.pkl')\n",
    "\n",
    "# load Roman dataset\n",
    "filename = 'roman.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('Roman Vocabulary: %d' % len(vocab))\n",
    "print('Roman Sentences: %d' % len(lines))\n",
    "\n",
    "# removing stop words from vocabulary\n",
    "vocab = removeStopWordsRomanian(lines)\n",
    "print ('### After removing the stop words ###')\n",
    "print('Roman Vocabulary: %d' % len(vocab))\n",
    "print('Roman Sentences: %d' % len(lines))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "\n",
    "print ('### After Pruning the data ###')\n",
    "\n",
    "print('New Roman Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "print('New Roman Sentences: %d' % len(lines))\n",
    "lines = update_dataset(lines, vocab)\n",
    "# save updated dataset\n",
    "filename = 'roman_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english_french.pkl\n",
      "English Vocabulary: 106227\n",
      "English Sentences: 2049662\n",
      "### After removing the stop words ###\n",
      "English Vocabulary: 106080\n",
      "English Sentences: 2049662\n",
      "### After Pruning the data ###\n",
      "New English Vocabulary: 41913\n",
      "New English Sentences: 2049662\n",
      "Saved: english_french_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = 'K:\\IIT Life\\Lecture Notes\\SEM 2 - OSNA DAA ADM\\ADM\\Project Reports\\Europal DS\\en-fr.txt\\Europarl.en-fr.en'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "sentences = clean_lines(sentences)\n",
    "save_clean_sentences(sentences, 'english_french.pkl')\n",
    "\n",
    "# load English dataset\n",
    "filename = 'english_french.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "print('English Sentences: %d' % len(lines))\n",
    "\n",
    "# removing stop words from vocabulary\n",
    "vocab = removeStopWordsEnglish(lines)\n",
    "print ('### After removing the stop words ###')\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "print('English Sentences: %d' % len(lines))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print ('### After Pruning the data ###')\n",
    "print('New English Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "print('New English Sentences: %d' % len(lines))\n",
    "# save updated dataset\n",
    "filename = 'english_french_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: french_english.pkl\n",
      "French Vocabulary: 142691\n",
      "French Sentences: 2049662\n",
      "### After removing the stop words ###\n",
      "French Vocabulary: 142562\n",
      "French Sentences: 2049662\n",
      "### After Pruning the data ###\n",
      "New French Vocabulary: 59138\n",
      "New French Sentences: 2049662\n",
      "Saved: french_english_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = 'K:\\IIT Life\\Lecture Notes\\SEM 2 - OSNA DAA ADM\\ADM\\Project Reports\\Europal DS\\en-fr.txt\\Europarl.en-fr.fr'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "sentences = clean_lines(sentences)\n",
    "save_clean_sentences(sentences, 'french_english.pkl')\n",
    "\n",
    "# load English dataset\n",
    "filename = 'french_english.pkl'\n",
    "lines = load_clean_sentences(filename)\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "print('French Vocabulary: %d' % len(vocab))\n",
    "print('French Sentences: %d' % len(lines))\n",
    "\n",
    "# removing stop words from vocabulary\n",
    "vocab = removeStopWordsFrench(lines)\n",
    "print ('### After removing the stop words ###')\n",
    "print('French Vocabulary: %d' % len(vocab))\n",
    "print('French Sentences: %d' % len(lines))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print ('### After Pruning the data ###')\n",
    "print('New French Vocabulary: %d' % len(vocab))\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "print('New French Sentences: %d' % len(lines))\n",
    "# save updated dataset\n",
    "filename = 'french_english_vocab.pkl'\n",
    "save_clean_sentences(lines, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency table for all words\n",
    "def removeStopWordsEnglish(lines):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Huahahaha': 1, 'makkar': 1, 'name': 2, 'pranav': 2, 'world!': 1})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopWordsEnglish(['this is my world! Huahahaha is my','this is name pranav', 'my name is pranav makkar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a frequency table for all words\n",
    "def removeStopWordsFrench(lines):\n",
    "    stopWords = set(stopwords.words('french'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Ceci': 1, 'Huahahaha': 1, 'monde!': 1})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopWordsFrench(['Ceci est mon monde! Huahahaha est mon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopWordsRomanian(lines):\n",
    "    wordsFiltered = []\n",
    "    stopWords = set(stopwords.words('romanian'))\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        wordsFiltered = []\n",
    "        tokens = line.split()\n",
    "        for w in tokens:\n",
    "            if w not in stopWords:\n",
    "                wordsFiltered.append(w)\n",
    "        vocab.update(wordsFiltered)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Dle': 1,\n",
       "         'Makkar': 2,\n",
       "         'Numele': 2,\n",
       "         'permiteți-mi': 1,\n",
       "         'pranav': 2,\n",
       "         'președinte,': 1,\n",
       "         'pun': 1,\n",
       "         'întrebare': 1})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopWordsRomanian(['Dle președinte, permiteți-mi să vă pun o întrebare','Numele meu este pranav Makkar','Numele este pranav Makkar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
