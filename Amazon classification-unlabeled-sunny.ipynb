{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn import metrics\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalueofnode(node):\n",
    "    \"\"\" return node text or None \"\"\"\n",
    "    return node.text if node is not None else None\n",
    "\n",
    "def read_w2v(language='english'):\n",
    "    if(language == 'french'):\n",
    "        path = 'wiki.multi.fr.vec'\n",
    "    elif(language == 'english'):\n",
    "        path = 'wiki.multi.en.vec'\n",
    "    elif(language == 'german'):\n",
    "        path = 'wiki.multi.de.vec'\n",
    "    t0 = time.time()\n",
    "    w2v = {}\n",
    "    count = 0\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf8\") as lines:\n",
    "        for line in lines:\n",
    "            lineArr = line.split()\n",
    "            if(count!=0):\n",
    "                x = []\n",
    "                for value in lineArr[len(lineArr)-300:]:\n",
    "                        x.append(float(value))\n",
    "                w2v[' '.join(lineArr[0:len(lineArr)-300])]=  np.array(x)\n",
    "            count+=1\n",
    "    print(count)\n",
    "    print(time.time()-t0)\n",
    "    return w2v\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "negation = {'arent','isnt','wasnt','werent','cant','couldnt','mustnt','shouldnt','wont','wouldnt','didnt','doesnt','dont','hasnt','havent','hadnt'}\n",
    "\n",
    "def tokenize(doc):    \n",
    "    doc = doc.lower()\n",
    "    token_list = doc.split()\n",
    "    tokenized_list=[]\n",
    "    for token in token_list:\n",
    "        new_token=''\n",
    "        for i in range(0,len(token)):\n",
    "            if(token[i] not in string.punctuation):\n",
    "                new_token+=token[i]\n",
    "        tokenized_list.append(new_token)\n",
    "    return tokenized_list\n",
    "\n",
    "def cleaningTextTokenizing(line, lang = 'english'):\n",
    "    if (lang == 'english'):\n",
    "        re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        line = tokenize(line)\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    else:\n",
    "        line = tokenize(line)\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    return line\n",
    "\n",
    "# def data_parse(parsed_xml_data, language):\n",
    "#     X_data = []\n",
    "#     y_data = []\n",
    "# #     X_refer = []\n",
    "#     for node in parsed_xml_data.getroot():\n",
    "#         try:\n",
    "#             summary = node.find('summary')\n",
    "#             rating = node.find('rating')\n",
    "#             text = node.find('text')\n",
    "#             tokens_summary = getvalueofnode(summary)\n",
    "#             tokens_text = getvalueofnode(text)\n",
    "#             if(tokens_summary == None and tokens_text == None ):\n",
    "#                 tokens = []\n",
    "#             elif(tokens_text == None):\n",
    "#                 tokens = cleaningTextTokenizing(tokens_summary,language)\n",
    "#             elif(tokens_summary == None):\n",
    "#                 tokens = cleaningTextTokenizing(tokens_text,language)\n",
    "#             else:\n",
    "#                 tokens = [*cleaningTextTokenizing(tokens_summary,language), *cleaningTextTokenizing(tokens_text,language)]\n",
    "\n",
    "#             lemmaStr = get_lemmatized_text(tokens)\n",
    "#             X_data.append(lemmaStr)\n",
    "#     #     X_refer.append(tokens_summary + \" \" + tokens_text)\n",
    "#             if(float(getvalueofnode(rating))>3):\n",
    "#                 y_data.append('positive')\n",
    "#             else:\n",
    "#                 y_data.append('negative')\n",
    "#         except:\n",
    "#             print('translation error')\n",
    "#     return X_data, y_data\n",
    "\n",
    "# Stop word removal of tokenized input data\n",
    "def get_lemmatized_text(tokenized_review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedStr = []\n",
    "    for word in tokenized_review:\n",
    "        lemmatizedStr.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatizedStr\n",
    "\n",
    "def data_parse(parsed_xml_data, language):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "#     X_refer = []\n",
    "    for node in parsed_xml_data.getroot():\n",
    "        \n",
    "        summary = node.find('summary')\n",
    "        rating = node.find('rating')\n",
    "        text = node.find('text')\n",
    "        tokens_summary = getvalueofnode(summary)\n",
    "        tokens_text = getvalueofnode(text)\n",
    "        if(tokens_summary == None and tokens_text == None ):\n",
    "            tokens = []\n",
    "        elif(tokens_text == None):\n",
    "            tokens = cleaningTextTokenizing(tokens_summary,language)\n",
    "        elif(tokens_summary == None):\n",
    "            tokens = cleaningTextTokenizing(tokens_text,language)\n",
    "        else:\n",
    "            tokens = [*cleaningTextTokenizing(tokens_summary,language), *cleaningTextTokenizing(tokens_text,language)]\n",
    "        \n",
    "        lemmaStr = get_lemmatized_text(tokens)\n",
    "        X_data.append(lemmaStr)\n",
    "#     X_refer.append(tokens_summary + \" \" + tokens_text)\n",
    "        if(float(getvalueofnode(rating))>3):\n",
    "            y_data.append('positive')\n",
    "        else:\n",
    "            y_data.append('negative')\n",
    "        \n",
    "    return X_data, y_data\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x, min_df = 0.0005)\n",
    "        tfidf.fit(X)\n",
    "#         print ('Vocab Size' , len(tfidf.vocabulary_))\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self,tfidf.vocabulary_.items()\n",
    "\n",
    "    def transform(self, X):\n",
    "        np_ar = np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "        return np_ar\n",
    "    \n",
    "class CountEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        count_vect = CountVectorizer(analyzer=lambda x: x, min_df = 0.0005)\n",
    "        count_vect.fit(X)\n",
    "        print ('Vocab Size' , len(count_vect.vocabulary_))\n",
    "        return self,count_vect.vocabulary_.items()\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         doc2vec = []\n",
    "#         for words in X:\n",
    "#             vec2mean = []\n",
    "#             for w in set(words):\n",
    "#                 if w in self.word2vec:\n",
    "#                     weighted_word = self.word2vec[w]\n",
    "#                 else:\n",
    "#                     weighted_word = np.zeros(self.dim)\n",
    "#                 vec2mean.append(weighted_word)\n",
    "#             doc2vec.append(np.mean(vec2mean,axis=0))\n",
    "#         return np.array(doc2vec)\n",
    "\n",
    "    def transform(self, X):\n",
    "        np_ar = np.array([\n",
    "                np.mean([self.word2vec[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "        return np_ar\n",
    "\n",
    "\n",
    "def words_not_w2vec(vocab, w2v):\n",
    "    new_words = []\n",
    "    percent_not_words = 0.0\n",
    "    for word,i in list(vocab):\n",
    "        if(word not in w2v):\n",
    "            new_words.append(word)\n",
    "    percent_not_words = len(new_words) / len(vocab) * 100\n",
    "    return new_words, percent_not_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ['this','this']\n",
    "set(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_path = 'cls-acl10-unprocessed/'\n",
    "# x_path = '/Users/vjstark/Downloads/ADM stuff/cls-acl10-unprocessed/'\n",
    "#ENGLISH\n",
    "parsed_xml = ET.parse(x_path+'en/books/train.review')\n",
    "parsed_xml_test = ET.parse(x_path+'en/books/test.review')\n",
    "parsed_xml_unlabeled = ET.parse(x_path+'en/books/unlabeled.review')\n",
    "#FRENCH\n",
    "parsed_xml_train_fr = ET.parse(x_path+'fr/books/train.review')\n",
    "parsed_xml_test_fr = ET.parse(x_path+'fr/books/test.review')\n",
    "parsed_xml_unlabeled_fr = ET.parse(x_path+'fr/books/unlabeled.review')\n",
    "#GERMAN\n",
    "parsed_xml_train_de = ET.parse(x_path+'de/books/train.review')\n",
    "parsed_xml_test_de = ET.parse(x_path+'de/books/test.review')\n",
    "parsed_xml_unlabeled_de = ET.parse(x_path+'de/books/unlabeled.review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "53.79565095901489\n"
     ]
    }
   ],
   "source": [
    "w2v = read_w2v('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "54.02349257469177\n"
     ]
    }
   ],
   "source": [
    "w2v_fr = read_w2v('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "57.280484676361084\n"
     ]
    }
   ],
   "source": [
    "w2v_de = read_w2v('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "X, y = data_parse(parsed_xml,'english')\n",
    "X_test, y_test = data_parse(parsed_xml_test,'english')\n",
    "X_unlabeled, y_unlabeled = data_parse(parsed_xml_unlabeled,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#French\n",
    "# X_train_fr, y_train_fr = data_parse(parsed_xml_train_fr,'french')\n",
    "X_test_fr, y_test_fr = data_parse(parsed_xml_test_fr,'french')\n",
    "X_unlabeled_fr, y_unlabeled_fr = data_parse(parsed_xml_unlabeled_fr, 'french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German\n",
    "# X_train_de, y_train_de = data_parse(parsed_xml_train_de,'german')\n",
    "X_test_de, y_test_de = data_parse(parsed_xml_test_de,'german')\n",
    "X_unlabeled_de, y_unlabeled_de = data_parse(parsed_xml_unlabeled_de, 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN-ul: 50000, FR-ul: 32870, DE-ul: 165470\n"
     ]
    }
   ],
   "source": [
    "print(f'EN-ul: {len(X_unlabeled)}, FR-ul: {len(X_unlabeled_fr)}, DE-ul: {len(X_unlabeled_de)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(X_unlabeled)\n",
    "len(X_test_fr)\n",
    "# X_test_fr = pickle.load(open('X_test_fr.pkl', 'rb')) \n",
    "# y_test_fr = pickle.load(open('y_test_fr.pkl', 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "cpos = 0\n",
    "cneg = 0\n",
    "for i in y_test_fr:\n",
    "    if i == 'positive':\n",
    "        cpos += 1\n",
    "    else:\n",
    "        cneg += 1\n",
    "        \n",
    "print(cpos, cneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_predict(w2v_lang, X_data, y_data,clf):\n",
    "    t0 = time.time()\n",
    "    vectorizer_data = CountEmbeddingVectorizer(w2v_lang)\n",
    "    X ,vocab = vectorizer_data.fit(X_data)\n",
    "    X_vect_data = vectorizer_data.transform(X_data)\n",
    "    result = clf.predict(X_vect_data)\n",
    "    print ('Accuracy Score ', metrics.accuracy_score(y_data, result))\n",
    "    print ('F1 Score ',metrics.f1_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print ('Precision Score ',metrics.precision_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print ('Recall Score ',metrics.recall_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print ('Confusion matrix ',metrics.confusion_matrix(y_data, result))\n",
    "    print(time.time()-t0)\n",
    "    return words_not_w2vec(vocab,w2v_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En-En, En-Fr, En-De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 10867\n",
      "175 1.6103800496917273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountEmbeddingVectorizer(w2v)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled)\n",
    "X_t = vectorizer.transform(X_unlabeled)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_en, percentage = words_not_w2vec(vocab,w2v)\n",
    "print(len(unknown_words_list_en),percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 20857\n",
      "Accuracy Score  0.855\n",
      "F1 Score  0.8570019723865878\n",
      "Precision Score  0.8453307392996109\n",
      "Recall Score  0.869\n",
      "Confusion matrix  [[869 131]\n",
      " [159 841]]\n",
      "1.2595529556274414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.81882\n",
    "# F1 Score  0.8223063494242953\n",
    "# Precision Score  0.8067818790654709\n",
    "# Recall Score  0.83844\n",
    "# 31.686428785324097\n",
    "#60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['super', 'recettes', 'faciles', 'à', 'réaliser', 'recettes', 'appréciées', 'de', 'toute', 'la', 'famille', 'petits', 'et', 'grandsde', 'plus', 'on', 'peut', 'faire', 'son', 'régime', 'en', 'ayant', 'de', 'invités', 'il', 'ny', 'voient', 'que', 'du', 'feupour', 'la', 'vinaigrette', 'il', 'ne', 'faut', 'surtout', 'pa', 'dire', 'quelle', 'est', 'faite', 'avec', 'de', 'lhuile', 'de', 'parafine', 'alors', 'elle', 'est', 'excellente', 'sinonle', 'régime', 'est', 'super', 'efficace', 'il', 'ne', 'fatigue', 'pa', 'du', 'toutjencourage', 'ceux', 'qui', 'ont', 'de', 'kilo', 'en', 'tropà', 'le', 'faireil', 'ne', 'faut', 'pa', 'beaucoup', 'de', 'volonté', 'car', 'on', 'mange', 'toujours', 'à', 'sa', 'faim']\n"
     ]
    }
   ],
   "source": [
    "print(X_test_fr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 22160\n",
      "Accuracy Score  0.5305\n",
      "F1 Score  0.12160898035547242\n",
      "Precision Score  0.9420289855072463\n",
      "Recall Score  0.065\n",
      "Confusion matrix  [[ 65 935]\n",
      " [  4 996]]\n",
      "0.8013656139373779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.7598722239123821\n",
    "# F1 Score  0.7549594858899132\n",
    "# Precision Score  0.7707276876267748\n",
    "# Recall Score  0.7398235473075753\n",
    "# 21.79134178161621\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 32891\n",
      "Accuracy Score  0.741\n",
      "F1 Score  0.7825356842989084\n",
      "Precision Score  0.6743849493487699\n",
      "Recall Score  0.932\n",
      "Confusion matrix  [[932  68]\n",
      " [450 550]]\n",
      "1.2705869674682617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.7439596301444371\n",
    "# F1 Score  0.7789966771516356\n",
    "# Precision Score  0.6852286909918508\n",
    "# Recall Score  0.9024959207107028\n",
    "# 93.78028178215027\n",
    "# 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fr-Fr, Fr-En, Fr-De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 10414\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountEmbeddingVectorizer(w2v_fr)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled_fr)\n",
    "X_t = vectorizer.transform(X_unlabeled_fr)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_fr = words_not_w2vec(vocab,w2v_fr)\n",
    "print(len(unknown_words_list_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 20857\n",
      "Accuracy Score  0.6605\n",
      "F1 Score  0.7421192556019749\n",
      "Precision Score  0.5982853643600735\n",
      "Recall Score  0.977\n",
      "Confusion matrix  [[977  23]\n",
      " [656 344]]\n",
      "1.2004690170288086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.51658\n",
    "# F1 Score  0.5918922113225387\n",
    "# Precision Score  0.5121103222602039\n",
    "# Recall Score  0.70112\n",
    "# 37.17140984535217\n",
    "# 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 22160\n",
      "Accuracy Score  0.8415\n",
      "F1 Score  0.8443789887088856\n",
      "Precision Score  0.8293153326904532\n",
      "Recall Score  0.86\n",
      "Confusion matrix  [[860 140]\n",
      " [177 823]]\n",
      "0.8081769943237305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.52299969577122\n",
    "# F1 Score  0.6233635206226429\n",
    "# Precision Score  0.5150035722791141\n",
    "# Recall Score  0.7894736842105263\n",
    "# 14.635821104049683\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 32891\n",
      "Accuracy Score  0.5965\n",
      "F1 Score  0.7100251527128997\n",
      "Precision Score  0.554122265844083\n",
      "Recall Score  0.988\n",
      "Confusion matrix  [[988  12]\n",
      " [795 205]]\n",
      "1.1001012325286865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.46508732700791683\n",
    "# F1 Score  0.46738554854859676\n",
    "# Precision Score  0.46538604417069107\n",
    "# Recall Score  0.46940230857557264\n",
    "# 105.32542681694031\n",
    "# 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-De, De-En, De-Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 12785\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountEmbeddingVectorizer(w2v_de)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled_de)\n",
    "X_t = vectorizer.transform(X_unlabeled_de)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_de = words_not_w2vec(vocab,w2v_de)\n",
    "print(len(unknown_words_list_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 20857\n",
      "Accuracy Score  0.669\n",
      "F1 Score  0.5284900284900285\n",
      "Precision Score  0.9183168316831684\n",
      "Recall Score  0.371\n",
      "Confusion matrix  [[371 629]\n",
      " [ 33 967]]\n",
      "1.314728021621704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.7691\n",
    "# F1 Score  0.7481072589618833\n",
    "# Precision Score  0.8229251668026688\n",
    "# Recall Score  0.68576\n",
    "# 35.73703718185425\n",
    "# 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 22160\n",
      "Accuracy Score  0.624\n",
      "F1 Score  0.4413075780089153\n",
      "Precision Score  0.8583815028901735\n",
      "Recall Score  0.297\n",
      "Confusion matrix  [[297 703]\n",
      " [ 49 951]]\n",
      "0.8092331886291504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.7205962884088835\n",
    "# F1 Score  0.7501904036557501\n",
    "# Precision Score  0.6783412858478036\n",
    "# Recall Score  0.8390629753574688\n",
    "# 14.586955070495605\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 32891\n",
      "Accuracy Score  0.848\n",
      "F1 Score  0.8472361809045227\n",
      "Precision Score  0.8515151515151516\n",
      "Recall Score  0.843\n",
      "Confusion matrix  [[843 157]\n",
      " [147 853]]\n",
      "1.1001272201538086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.81372454221309\n",
    "# F1 Score  0.8170492114650665\n",
    "# Precision Score  0.8027221198479159\n",
    "# Recall Score  0.8318970206079652\n",
    "# 97.7156879901886\n",
    "# 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.62844e-02, -5.95874e-02, -4.77496e-02,  9.80034e-03,\n",
       "       -1.16180e-01, -6.10507e-02,  3.44817e-03, -5.22143e-04,\n",
       "       -2.17907e-02,  8.00969e-02, -4.13871e-02,  4.40307e-02,\n",
       "       -5.94686e-02,  5.27718e-02,  2.43733e-02,  1.48229e-02,\n",
       "       -6.33436e-02, -4.37056e-02, -1.10108e-01,  6.81111e-02,\n",
       "       -4.49096e-02,  1.12009e-02, -1.33384e-01, -6.53590e-02,\n",
       "       -5.25449e-03, -5.62300e-02, -1.68066e-02, -1.21397e-02,\n",
       "       -3.22654e-04,  4.80870e-02,  3.34291e-03,  1.09416e-01,\n",
       "       -9.66857e-02, -1.74348e-02, -5.12598e-03, -7.13200e-02,\n",
       "        4.03671e-02, -6.44001e-02,  3.86578e-02,  2.84616e-02,\n",
       "        2.31704e-02,  2.75342e-02, -8.73379e-03,  3.39149e-02,\n",
       "       -7.58863e-03,  5.24873e-02,  2.68174e-02,  3.49733e-02,\n",
       "       -6.78260e-02, -4.19187e-02, -7.92524e-03, -5.71953e-02,\n",
       "       -2.19931e-02, -4.17515e-02, -1.00213e-01,  1.85027e-02,\n",
       "        3.74559e-02,  3.81762e-02, -5.96340e-03, -3.39235e-02,\n",
       "       -1.20289e-01, -8.86334e-02,  9.12358e-02, -1.90872e-02,\n",
       "       -4.08261e-02, -3.44919e-02,  1.13402e-02,  1.22914e-01,\n",
       "       -6.62012e-02, -3.73825e-02,  2.85790e-02,  7.49926e-02,\n",
       "        2.26806e-02, -2.63756e-02, -3.37682e-02,  3.94536e-02,\n",
       "        6.00018e-02,  2.91549e-02,  3.73332e-02,  7.52995e-02,\n",
       "        4.58763e-02, -4.57858e-02, -9.35807e-02, -2.31143e-03,\n",
       "       -1.11737e-01,  7.15514e-02, -7.41354e-02, -8.39282e-06,\n",
       "        1.37994e-02, -4.03501e-02,  8.99330e-05,  4.04247e-03,\n",
       "        4.17179e-02, -4.09905e-02, -3.08266e-03,  9.90052e-02,\n",
       "       -1.32914e-05, -4.63417e-03, -4.02296e-02, -2.06755e-02,\n",
       "        8.83012e-02,  1.19860e-02, -5.35900e-02, -2.93390e-02,\n",
       "       -3.86085e-03, -9.32689e-02,  7.50710e-03,  4.12002e-02,\n",
       "       -9.37941e-03,  4.95818e-02,  3.01400e-02,  3.71114e-02,\n",
       "        1.41523e-02, -1.85117e-02,  3.13799e-03, -5.65968e-02,\n",
       "        1.04595e-01, -7.98186e-03, -5.55718e-03,  1.09022e-02,\n",
       "        5.19839e-02,  5.84856e-02, -6.96303e-02,  9.59401e-02,\n",
       "       -1.53039e-02, -7.02686e-02,  1.65878e-02, -2.40340e-02,\n",
       "       -1.52916e-02,  7.22935e-02, -1.10809e-01, -3.11857e-03,\n",
       "       -1.16326e-01, -1.19327e-02, -1.30260e-02,  1.65177e-02,\n",
       "       -4.31403e-02,  2.92896e-03, -8.43732e-03, -1.73739e-02,\n",
       "       -3.97328e-02,  5.44944e-02,  7.76686e-02, -1.15542e-01,\n",
       "        5.99515e-02,  6.35385e-02,  3.74126e-02, -1.31930e-01,\n",
       "        9.38676e-03,  8.47648e-02,  1.99336e-02, -4.41317e-02,\n",
       "       -2.50060e-02,  6.17691e-03, -1.15227e-01, -3.82073e-02,\n",
       "       -3.09367e-05, -9.55210e-04,  1.38130e-02, -3.07905e-02,\n",
       "       -6.70028e-02, -7.06248e-02, -9.90146e-02,  8.32821e-02,\n",
       "       -5.31468e-02,  7.67637e-02, -1.10748e-01, -2.45389e-02,\n",
       "        1.54481e-02,  2.25254e-02, -4.42182e-02, -3.37646e-02,\n",
       "       -1.55141e-02, -5.41949e-04, -1.10672e-02,  2.01100e-01,\n",
       "       -7.13448e-03,  5.71776e-02, -4.87516e-02, -6.95303e-02,\n",
       "        3.16647e-02, -4.50338e-02, -7.17915e-02, -8.50004e-03,\n",
       "       -5.55758e-02,  8.32850e-02, -1.03224e-01,  4.57112e-02,\n",
       "        6.97383e-02, -7.09876e-02, -6.21494e-02, -7.78665e-02,\n",
       "       -4.97723e-02,  1.82287e-02, -2.77895e-02,  2.98701e-02,\n",
       "       -3.28010e-02, -7.57148e-02, -1.59833e-01,  5.78808e-02,\n",
       "       -9.63945e-05, -4.82605e-02,  7.50705e-02, -7.87649e-02,\n",
       "        6.26975e-03, -4.11437e-05, -3.24167e-02,  7.14974e-02,\n",
       "        2.61412e-02,  3.38997e-02,  1.12397e-01,  8.25990e-03,\n",
       "        4.48514e-02, -2.54158e-02,  8.64265e-03, -5.88263e-02,\n",
       "        7.81151e-03, -3.41593e-02,  2.94060e-03, -3.63148e-03,\n",
       "       -1.30568e-01, -2.00044e-02,  3.44668e-02,  4.52731e-02,\n",
       "       -2.57428e-02, -3.08051e-02, -1.81340e-03,  3.37569e-02,\n",
       "       -1.46738e-02,  4.43156e-02, -1.84872e-02,  1.04934e-02,\n",
       "        6.44637e-02,  1.32023e-01,  4.18604e-02,  4.36360e-02,\n",
       "        9.10304e-02,  7.21180e-02, -4.78889e-03, -2.63800e-02,\n",
       "        5.23137e-02, -1.64161e-02,  3.38164e-02,  6.81360e-02,\n",
       "        1.60136e-02,  3.29283e-02,  2.40776e-02,  7.45137e-02,\n",
       "       -6.21662e-02,  8.94243e-02,  2.76790e-03, -8.05472e-03,\n",
       "       -1.11744e-01, -5.87353e-02,  8.61082e-03,  1.31475e-03,\n",
       "        5.83408e-02, -1.24892e-01, -3.29250e-02, -8.99926e-02,\n",
       "        4.83092e-02,  9.10053e-03,  3.15943e-02, -8.88303e-02,\n",
       "       -9.91319e-02, -9.13202e-02,  4.76719e-03, -1.95009e-02,\n",
       "        2.52503e-02, -5.80850e-02, -6.09624e-02, -4.67410e-02,\n",
       "       -8.74290e-02,  2.99541e-02, -5.06449e-02, -2.84621e-02,\n",
       "        9.59654e-02,  1.05007e-01, -5.87451e-02,  3.59817e-02,\n",
       "       -3.09829e-02, -2.81456e-02,  9.94479e-03, -2.37140e-02,\n",
       "        2.57275e-02,  5.60904e-02,  9.52043e-02,  2.12899e-02,\n",
       "       -1.97220e-02,  1.86065e-02, -2.32605e-03,  1.27511e-01,\n",
       "       -4.34200e-02, -1.14508e-01,  1.56644e-02, -1.09773e-01,\n",
       "       -3.62849e-02,  9.42199e-02,  5.89382e-02, -1.27601e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_fr[\"arrêter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
