{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn import metrics\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from unicodedata import normalize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvalueofnode(node):\n",
    "    \"\"\" return node text or None \"\"\"\n",
    "    return node.text if node is not None else None\n",
    "\n",
    "def read_w2v(language='english'):\n",
    "    if(language == 'french'):\n",
    "        path = 'wiki.multi.fr.vec'\n",
    "    elif(language == 'english'):\n",
    "        path = 'wiki.multi.en.vec'\n",
    "    elif(language == 'german'):\n",
    "        path = 'wiki.multi.de.vec'\n",
    "    t0 = time.time()\n",
    "    w2v = {}\n",
    "    count = 0\n",
    "    \n",
    "    with open(\"/Users/vjstark/Downloads/ADM stuff/\"+ path, \"r\", encoding=\"utf8\") as lines:\n",
    "        for line in lines:\n",
    "            lineArr = line.split()\n",
    "            if(count!=0):\n",
    "                x = []\n",
    "                for value in lineArr[len(lineArr)-300:]:\n",
    "                        x.append(float(value))\n",
    "                w2v[' '.join(lineArr[0:len(lineArr)-300])]=  np.array(x)\n",
    "            count+=1\n",
    "    print(count)\n",
    "    print(time.time()-t0)\n",
    "    return w2v\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "negation = {'arent','isnt','wasnt','werent','cant','couldnt','mustnt','shouldnt','wont','wouldnt','didnt','doesnt','dont','hasnt','havent','hadnt'}\n",
    "\n",
    "def tokenize(doc):    \n",
    "    doc = doc.lower()\n",
    "    token_list = doc.split()\n",
    "    tokenized_list=[]\n",
    "    for token in token_list:\n",
    "        new_token=''\n",
    "        for i in range(0,len(token)):\n",
    "            if(token[i] not in string.punctuation):\n",
    "                new_token+=token[i]\n",
    "        tokenized_list.append(new_token)\n",
    "    return tokenized_list\n",
    "\n",
    "def cleaningTextTokenizing(line, lang = 'english'):\n",
    "    if (lang == 'english'):\n",
    "        re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        line = tokenize(line)\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    else:\n",
    "        line = tokenize(line)\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    return line\n",
    "\n",
    "# Stop word removal of tokenized input data\n",
    "def get_lemmatized_text(tokenized_review):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizedStr = []\n",
    "    for word in tokenized_review:\n",
    "        lemmatizedStr.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatizedStr\n",
    "\n",
    "def data_parse(parsed_xml_data, language):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "#     X_refer = []\n",
    "    for node in parsed_xml_data.getroot():\n",
    "        \n",
    "        summary = node.find('summary')\n",
    "        rating = node.find('rating')\n",
    "        text = node.find('text')\n",
    "        tokens_summary = getvalueofnode(summary)\n",
    "        tokens_text = getvalueofnode(text)\n",
    "        if(tokens_summary == None and tokens_text == None ):\n",
    "            tokens = []\n",
    "        elif(tokens_text == None):\n",
    "            tokens = cleaningTextTokenizing(tokens_summary,language)\n",
    "        elif(tokens_summary == None):\n",
    "            tokens = cleaningTextTokenizing(tokens_text,language)\n",
    "        else:\n",
    "            tokens = [*cleaningTextTokenizing(tokens_summary,language), *cleaningTextTokenizing(tokens_text,language)]\n",
    "        \n",
    "        lemmaStr = get_lemmatized_text(tokens)\n",
    "        X_data.append(lemmaStr)\n",
    "#     X_refer.append(tokens_summary + \" \" + tokens_text)\n",
    "        if(float(getvalueofnode(rating))>3):\n",
    "            y_data.append('positive')\n",
    "        else:\n",
    "            y_data.append('negative')\n",
    "        \n",
    "    return X_data, y_data\n",
    "\n",
    "def words_not_w2vec(vocab, w2v):\n",
    "    new_words = []\n",
    "    for word,i in list(vocab):\n",
    "        if(word not in w2v):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x, min_df = 0.001)\n",
    "        tfidf.fit(X)\n",
    "        print ('Vocab Size' , len(tfidf.vocabulary_))\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self,tfidf.vocabulary_.items()\n",
    "\n",
    "    def transform(self, X):\n",
    "        np_ar = np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "        return np_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_path = 'C:/Users/sunny/Desktop/adm project/amazon dataset/cls-acl10-unprocessed/'\n",
    "x_path = '/Users/vjstark/Downloads/ADM stuff/cls-acl10-unprocessed/'\n",
    "#ENGLISH\n",
    "parsed_xml = ET.parse(x_path+'en/books/train.review')\n",
    "parsed_xml_test = ET.parse(x_path+'en/books/test.review')\n",
    "parsed_xml_unlabeled = ET.parse(x_path+'en/books/unlabeled.review')\n",
    "#FRENCH\n",
    "parsed_xml_train_fr = ET.parse(x_path+'fr/books/train.review')\n",
    "parsed_xml_test_fr = ET.parse(x_path+'fr/books/test.review')\n",
    "parsed_xml_unlabeled_fr = ET.parse(x_path+'fr/books/unlabeled.review')\n",
    "#GERMAN\n",
    "parsed_xml_train_de = ET.parse(x_path+'de/books/train.review')\n",
    "parsed_xml_test_de = ET.parse(x_path+'de/books/test.review')\n",
    "parsed_xml_unlabeled_de = ET.parse(x_path+'de/books/unlabeled.review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "18.81849503517151\n"
     ]
    }
   ],
   "source": [
    "w2v = read_w2v('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "19.066630125045776\n"
     ]
    }
   ],
   "source": [
    "w2v_fr = read_w2v('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200001\n",
      "19.474637031555176\n"
     ]
    }
   ],
   "source": [
    "w2v_de = read_w2v('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "X, y = data_parse(parsed_xml,'english')\n",
    "X_test, y_test = data_parse(parsed_xml_test,'english')\n",
    "X_unlabeled, y_unlabeled = data_parse(parsed_xml_unlabeled,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#French\n",
    "X_train_fr, y_train_fr = data_parse(parsed_xml_train_fr,'french')\n",
    "X_test_fr, y_test_fr = data_parse(parsed_xml_test_fr,'french')\n",
    "X_unlabeled_fr, y_unlabeled_fr = data_parse(parsed_xml_unlabeled_fr, 'french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German\n",
    "X_train_de, y_train_de = data_parse(parsed_xml_train_de,'german')\n",
    "X_test_de, y_test_de = data_parse(parsed_xml_test_de,'german')\n",
    "X_unlabeled_de, y_unlabeled_de = data_parse(parsed_xml_unlabeled_de, 'german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN-ul: 50000, FR-ul: 32870, DE-ul: 165470\n"
     ]
    }
   ],
   "source": [
    "print(f'EN-ul: {len(X_unlabeled)}, FR-ul: {len(X_unlabeled_fr)}, DE-ul: {len(X_unlabeled_de)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(X_unlabeled)\n",
    "len(y_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "cpos = 0\n",
    "cneg = 0\n",
    "for i in y_unlabeled:\n",
    "    if i == 'positive':\n",
    "        cpos += 1\n",
    "    else:\n",
    "        cneg += 1\n",
    "        \n",
    "print(cpos, cneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_predict(w2v_lang, X_data, y_data,clf):\n",
    "    t0 = time.time()\n",
    "    vectorizer_data = TfidfEmbeddingVectorizer(w2v_lang)\n",
    "    X ,vocab = vectorizer_data.fit(X_data)\n",
    "    X_vect_data = vectorizer_data.transform(X_data)\n",
    "    result = clf.predict(X_vect_data)\n",
    "    print ('Accuracy Score ', metrics.accuracy_score(y_data, result))\n",
    "    print ('F1 Score ',metrics.f1_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print ('Precision Score ',metrics.precision_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print ('Recall Score ',metrics.recall_score(y_data, result,average=\"binary\", pos_label=\"negative\"))\n",
    "    print(time.time()-t0)\n",
    "    return words_not_w2vec(vocab,w2v_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En-En, En-Fr, En-De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 6996\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = TfidfEmbeddingVectorizer(w2v)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled)\n",
    "X_t = vectorizer.transform(X_unlabeled)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_en = words_not_w2vec(vocab,w2v)\n",
    "print(len(unknown_words_list_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 9113\n",
      "Accuracy Score  0.8405\n",
      "F1 Score  0.842779694430754\n",
      "Precision Score  0.8309037900874635\n",
      "Recall Score  0.855\n",
      "3.30194091796875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.81882\n",
    "# F1 Score  0.8223063494242953\n",
    "# Precision Score  0.8067818790654709\n",
    "# Recall Score  0.83844\n",
    "# 31.686428785324097\n",
    "#60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 8933\n",
      "Accuracy Score  0.5715\n",
      "F1 Score  0.2668947818648418\n",
      "Precision Score  0.9230769230769231\n",
      "Recall Score  0.156\n",
      "2.461233377456665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.7598722239123821\n",
    "# F1 Score  0.7549594858899132\n",
    "# Precision Score  0.7707276876267748\n",
    "# Recall Score  0.7398235473075753\n",
    "# 21.79134178161621\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 11213\n",
      "Accuracy Score  0.7565\n",
      "F1 Score  0.7821029082774049\n",
      "Precision Score  0.7076923076923077\n",
      "Recall Score  0.874\n",
      "4.295766115188599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.7439596301444371\n",
    "# F1 Score  0.7789966771516356\n",
    "# Precision Score  0.6852286909918508\n",
    "# Recall Score  0.9024959207107028\n",
    "# 93.78028178215027\n",
    "# 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fr-Fr, Fr-En, Fr-De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 6261\n",
      "534\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfEmbeddingVectorizer(w2v_fr)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled_fr)\n",
    "X_t = vectorizer.transform(X_unlabeled_fr)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_fr = words_not_w2vec(vocab,w2v_fr)\n",
    "print(len(unknown_words_list_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 9113\n",
      "Accuracy Score  0.7365\n",
      "F1 Score  0.7786644267114657\n",
      "Precision Score  0.6712527154236061\n",
      "Recall Score  0.927\n",
      "2.850640296936035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.51658\n",
    "# F1 Score  0.5918922113225387\n",
    "# Precision Score  0.5121103222602039\n",
    "# Recall Score  0.70112\n",
    "# 37.17140984535217\n",
    "# 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 8933\n",
      "Accuracy Score  0.841\n",
      "F1 Score  0.8431952662721893\n",
      "Precision Score  0.8317120622568094\n",
      "Recall Score  0.855\n",
      "2.5710132122039795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.52299969577122\n",
    "# F1 Score  0.6233635206226429\n",
    "# Precision Score  0.5150035722791141\n",
    "# Recall Score  0.7894736842105263\n",
    "# 14.635821104049683\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 11213\n",
      "Accuracy Score  0.6965\n",
      "F1 Score  0.7563227619429946\n",
      "Precision Score  0.6317907444668008\n",
      "Recall Score  0.942\n",
      "3.1197590827941895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.46508732700791683\n",
    "# F1 Score  0.46738554854859676\n",
    "# Precision Score  0.46538604417069107\n",
    "# Recall Score  0.46940230857557264\n",
    "# 105.32542681694031\n",
    "# 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-De, De-En, De-Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 7575\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfEmbeddingVectorizer(w2v_de)\n",
    "vect,vocab = vectorizer.fit(X_unlabeled_de)\n",
    "X_t = vectorizer.transform(X_unlabeled_de)\n",
    "clfLSVC = LinearSVC()\n",
    "LSVC = CalibratedClassifierCV(clfLSVC)\n",
    "unknown_words_list_de = words_not_w2vec(vocab,w2v_de)\n",
    "print(len(unknown_words_list_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "            cv='warn', method='sigmoid')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC.fit(X_t, y_unlabeled_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 9113\n",
      "Accuracy Score  0.772\n",
      "F1 Score  0.7472283813747228\n",
      "Precision Score  0.8383084577114428\n",
      "Recall Score  0.674\n",
      "2.671334981918335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_en = vectorize_predict(w2v, X_test,y_test, LSVC)\n",
    "len(unknown_words_test_en)\n",
    "\n",
    "# Vocab Size 6996\n",
    "# Accuracy Score  0.7691\n",
    "# F1 Score  0.7481072589618833\n",
    "# Precision Score  0.8229251668026688\n",
    "# Recall Score  0.68576\n",
    "# 35.73703718185425\n",
    "# 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 8933\n",
      "Accuracy Score  0.709\n",
      "F1 Score  0.663972286374134\n",
      "Precision Score  0.7855191256830601\n",
      "Recall Score  0.575\n",
      "2.189793109893799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1057"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_fr = vectorize_predict(w2v_fr, X_test_fr,y_test_fr, LSVC)\n",
    "len(unknown_words_test_fr)\n",
    "\n",
    "# Vocab Size 6261\n",
    "# Accuracy Score  0.7205962884088835\n",
    "# F1 Score  0.7501904036557501\n",
    "# Precision Score  0.6783412858478036\n",
    "# Recall Score  0.8390629753574688\n",
    "# 14.586955070495605\n",
    "# 534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 11213\n",
      "Accuracy Score  0.8325\n",
      "F1 Score  0.8314041268243583\n",
      "Precision Score  0.8368794326241135\n",
      "Recall Score  0.826\n",
      "3.032680034637451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "634"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words_test_de = vectorize_predict(w2v_de, X_test_de,y_test_de, LSVC)\n",
    "len(unknown_words_test_de)\n",
    "\n",
    "# Vocab Size 7575\n",
    "# Accuracy Score  0.81372454221309\n",
    "# F1 Score  0.8170492114650665\n",
    "# Precision Score  0.8027221198479159\n",
    "# Recall Score  0.8318970206079652\n",
    "# 97.7156879901886\n",
    "# 100"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
